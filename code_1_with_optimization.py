# -*- coding: utf-8 -*-
"""code 1 with optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VbfBDj79nKzDfXk3Q9a4IXi4XhQ69Ccz
"""

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Function to clean text data
def clean_text(text):
    if not isinstance(text, str):
        return ""  # Handle non-string values
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\@w+|\#','', text)  # Remove @ and #
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuations
    return text

# Load datasets
training_data_path = '/content/twitter_training.csv'  # Replace with your file path
validation_data_path = '/content/twitter_validation.csv'  # Replace with your file path

training_data = pd.read_csv(training_data_path)
validation_data = pd.read_csv(validation_data_path)

# Apply text cleaning
training_data['cleaned_text'] = training_data.iloc[:, 3].apply(clean_text)
validation_data['cleaned_text'] = validation_data.iloc[:, 3].apply(clean_text)

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed
X_train = tfidf_vectorizer.fit_transform(training_data['cleaned_text'])
X_val = tfidf_vectorizer.transform(validation_data['cleaned_text'])

# Extracting target labels
y_train = training_data.iloc[:, 2]
y_val = validation_data.iloc[:, 2]

# Logistic Regression Model
model = LogisticRegression(max_iter=500)  # Adjust max_iter as needed
model.fit(X_train, y_train)

# Predictions and Evaluation
y_pred = model.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

# Output results
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

import pandas as pd
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Download NLTK resources
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Lemmatization function with POS tagging
def lemmatize_with_pos(text):
    lemmatizer = WordNetLemmatizer()
    pos_tagged = nltk.pos_tag(text.split())

    def get_wordnet_pos(tag):
        if tag.startswith('J'):
            return wordnet.ADJ
        elif tag.startswith('V'):
            return wordnet.VERB
        elif tag.startswith('N'):
            return wordnet.NOUN
        elif tag.startswith('R'):
            return wordnet.ADV
        else:
            return wordnet.NOUN

    return ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tagged])

# Text cleaning and preprocessing
def clean_and_preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#','', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    return lemmatize_with_pos(text)

# Load datasets
training_data = pd.read_csv('/content/twitter_training.csv')
validation_data = pd.read_csv('/content/twitter_validation.csv')

# Apply text cleaning and preprocessing
training_data['processed_text'] = training_data.iloc[:, 3].apply(clean_and_preprocess_text)
validation_data['processed_text'] = validation_data.iloc[:, 3].apply(clean_and_preprocess_text)

# Extracting features and labels
X_train, X_val = training_data['processed_text'], validation_data['processed_text']
y_train, y_val = training_data.iloc[:, 2], validation_data.iloc[:, 2]

# Creating a pipeline for grid search
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

# Parameters for grid search
parameters = {
    'tfidf__max_df': (0.5, 0.75, 1.0),
    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams
    'clf__alpha': (1e-2, 1e-3),
}

# Grid search
grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model and predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_val)

# Evaluation
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

# Output results
print("Best Model Parameters:", grid_search.best_params_)
print("Accuracy:", accuracy)
print("Classification Report:\n", report)